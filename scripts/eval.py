import json
import argparse
import os
import pandas as pd
import torch
from tqdm import tqdm
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

from core.retrieval import RetrievalSystem

def run_evaluation(dataset_name):
    # --- è·¯å¾„é…ç½® ---
    gt_path = f"benchmark_gt_{dataset_name}.json"
    
    # æŒ‡å‘ vectorstore æ ¹ç›®å½•
    base_dir = os.path.join("vectorstore", dataset_name)
    doc_path = os.path.join(base_dir, "doc_store.json")

    if not os.path.exists(gt_path):
        print(f"âŒ Ground truth not found: {gt_path}")
        return

    print(f"ğŸš€ Initializing Dual-Path Retrieval System for [{dataset_name}]...")
    
    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = RetrievalSystem(index_root_path=base_dir, doc_store_path=doc_path)

    # --- åŠ è½½æ•°æ® ---
    with open(gt_path, "r", encoding='utf-8') as f:
        raw_data = json.load(f)

    # è¿‡æ»¤æ— æ•ˆæ•°æ®
    ground_truth = [item for item in raw_data if item and item.get('query')]
    
    # ğŸ’¡ è°ƒè¯•æ¨¡å¼ï¼šå¦‚æœåªæƒ³æµ‹å‰ 10 æ¡çœ‹çœ‹é€šä¸é€šï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š
    # ground_truth = ground_truth[:10]

    print(f"ğŸ§ª Starting evaluation on {len(ground_truth)} queries...")

    results = []
    
    # --- è¯„æµ‹å¾ªç¯ ---
    for item in tqdm(ground_truth):
        query = item['query']
        target_img = item['image_filename'].strip() # å»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼

        try:
            # æ‰§è¡Œæ£€ç´¢ï¼šè·å– Top 10 ç”¨äºè®¡ç®— Recall@10 å’Œ MRR
            search_res = retriever.search_and_rerank(query, top_k_final=10)

            # æå–æ–‡ä»¶å (æ ‡å‡†åŒ–å¤„ç†)
            retrieved_files = [os.path.basename(r['path']).strip() for r in search_res]

            # --- è®¡ç®—æŒ‡æ ‡ ---
            
            # 1. Recall@5
            top5_files = retrieved_files[:5]
            hit_5 = 1 if target_img in top5_files else 0
            
            # 2. Recall@10
            hit_10 = 1 if target_img in retrieved_files else 0
            
            # 3. MRR@10 (Mean Reciprocal Rank)
            # å¦‚æœåœ¨ç¬¬ 1 ä½ï¼Œå¾— 1 åˆ†ï¼›ç¬¬ 2 ä½ï¼Œå¾— 0.5 åˆ†... æ²¡æ‰¾åˆ°å¾— 0 åˆ†
            mrr = 0.0
            if target_img in retrieved_files:
                rank = retrieved_files.index(target_img) + 1
                mrr = 1.0 / rank

            results.append({
                "id": item['id'],
                "recall@5": hit_5,
                "recall@10": hit_10,
                "mrr": mrr
            })

        except Exception as e:
            print(f"âš ï¸ Error processing query {item.get('id')}: {e}")

    # --- ç»Ÿè®¡è¾“å‡º ---
    if results:
        df = pd.DataFrame(results)
        
        print(f"\nğŸ† Benchmark Report for [{dataset_name}]")
        print("="*40)
        print(f"âœ… Recall@5  : {df['recall@5'].mean():.4f}")
        print(f"âœ… Recall@10 : {df['recall@10'].mean():.4f}")
        print(f"ğŸ¥‡ MRR       : {df['mrr'].mean():.4f}")
        print("="*40)
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSVï¼Œæ–¹ä¾¿åˆ†æ Bad Case
        output_csv = f"eval_results_{dataset_name}.csv"
        df.to_csv(output_csv, index=False)
        print(f"ğŸ“„ Detailed results saved to {output_csv}")
        
    else:
        print("âŒ No results generated.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, required=True, help="Dataset name (e.g., energy)")
    args = parser.parse_args()
    
    run_evaluation(args.dataset)