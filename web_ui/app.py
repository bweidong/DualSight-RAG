import streamlit as st
import os

# å…¨å±€æ˜¾å¡å¯è§æ€§ (Retrieval: 0, LLM: 1,2)
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2"

from core.retrieval import RetrievalSystem
from core.llm_engine import VLLMEngine

st.set_page_config(layout="wide", page_title="DualSight-RAG v2", page_icon="ğŸ‘ï¸")

# --- CSS ---
st.markdown("""
<style>
    .score-tag {
        font-size: 0.8em; color: #555; background: #eee; 
        padding: 2px 5px; border-radius: 4px; margin-right: 5px;
    }
</style>
""", unsafe_allow_html=True)

# --- ç³»ç»ŸåŠ è½½ ---
@st.cache_resource
def load_system():
    # è¿™é‡Œçš„è·¯å¾„éœ€å¯¹åº” ingest ç”Ÿæˆçš„è·¯å¾„
    retriever = RetrievalSystem(
        index_root_path="vectorstore/energy",
        doc_store_path="vectorstore/energy/doc_store.json"
    )
    llm = VLLMEngine()
    return retriever, llm

if "system_loaded" not in st.session_state:
    with st.spinner("ğŸš€ Booting SigLIP Fragment-Aggregation System..."):
        try:
            retriever, llm = load_system()
            st.session_state.retriever = retriever
            st.session_state.llm = llm
            st.session_state.system_loaded = True
            st.success("âœ… System Online")
        except Exception as e:
            st.error(f"Init Failed: {e}")
            st.stop()
else:
    retriever = st.session_state.retriever
    llm = st.session_state.llm

# --- UI é€»è¾‘ ---
st.title("ğŸ‘ï¸ DualSight-RAG (Fragment Aggregation)")

if prompt := st.chat_input("Ask about charts or documents..."):
    st.chat_message("user").write(prompt)

    # 1. æ£€ç´¢ä¸èåˆ
    with st.status("ğŸ” Retrieving & Aggregating Fragments...", expanded=True) as status:
        # è¿™ä¸€æ­¥å†…éƒ¨å®Œæˆäº†ï¼šç¢ç‰‡å¬å› -> Docèšåˆ -> å…¬å¼æ‰“åˆ† -> åŒè·¯èåˆ
        results = retriever.search_and_rerank(prompt, top_k_final=5)
        
        evidence_text = ""
        if results:
            status.write(f"âœ… Found {len(results)} docs after fusion.")
            cols = st.columns(3)
            for i, res in enumerate(results):
                # è·å–åˆ†é¡¹å¾—åˆ†
                s_final = res['final_score']
                s_txt = res['scores']['text']
                s_vis = res['scores']['visual']
                
                # æ„é€  Prompt ä¸Šä¸‹æ–‡
                evidence_text += f"\n[Doc {i+1}]: {res['content']}\n"
                
                # UI å±•ç¤º
                if i < 3:
                    with cols[i]:
                        st.image(res['path'])
                        st.markdown(f"**Rank {i+1}** (Score: {s_final:.3f})")
                        st.caption(f"Text: {s_txt:.3f} | Vis: {s_vis:.3f}")
                        with st.expander("Show Text"):
                            st.text(res['content'][:200] + "...")
        else:
            status.write("âš ï¸ No relevant documents found.")
            
        status.update(label="Retrieval Complete", state="complete")

    # 2. ç”Ÿæˆå›ç­”
    if evidence_text:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ§  Reasoning..."):
                final_prompt = f"""
                Context:
                {evidence_text}
                
                User Question: "{prompt}"
                
                Answer the question based on the Context.
                """
                response = llm.generate(final_prompt)
                st.write(response)